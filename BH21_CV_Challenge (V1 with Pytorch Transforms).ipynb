{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BH21_CV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtLPLbA0cxBE"
      },
      "source": [
        "## Downloading Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h7Cxnb-5XHg"
      },
      "source": [
        "# First, we need to install pycocotools. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n",
        "# The version by default in Colab has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "!pip install cython\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xealk6oLy1nH"
      },
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from torchvision import transforms\n",
        "from torchvision.ops import batched_nms\n",
        "from torchvision.transforms import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOh0f5W0zGNR"
      },
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.ops import MultiScaleRoIAlign"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ssnCCqtzfkn"
      },
      "source": [
        "In `references/detection/,` we have a number of helper functions to simplify training and evaluating detection models.\n",
        "Here, we will use `references/detection/utils.py` and `references/detection/coco_eval.py`.\n",
        "\n",
        "Let's copy those files (and their dependencies) in here so that they are available in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe2V9r4wzl_q"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/coco_eval.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bpxOrWJzJmh"
      },
      "source": [
        "import utils\n",
        "from coco_eval import CocoEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUiwwAIP6_o_"
      },
      "source": [
        "# check if cuda GPU is available, make sure you're using GPU runtime on Google Colab\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device) # you should output \"cuda\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNnVy01g2FRO"
      },
      "source": [
        "# mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9z4qZ8A8HTn"
      },
      "source": [
        "## Object Detection Dataset\n",
        "We will be providing the base dataset that will be used for the first task of the Object Detection competition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eghj9hRaFWUz"
      },
      "source": [
        "# You should have uploaded the training dataset onto your google drive.\n",
        "\n",
        "base_folder = '/content/drive/MyDrive/TIL2021' # Change this path to your folder for this competition\n",
        "training_path = os.path.join(base_folder, \"c1_release.zip\") # zip file for training dataset for challenge 1\n",
        "hidden_path = os.path.join(base_folder, \"c1_hidden.zip\") # zip file for hidden dataset for challenge 1\n",
        "c3_training_path = os.path.join(base_folder, \"c3_release.zip\") # zip file for training dataset for challenge 3\n",
        "c3_hidden_path = os.path.join(base_folder, \"c3_hidden.zip\") # zip file for training dataset for challenge 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM0SfOF03NT1"
      },
      "source": [
        "# unzip the training dataset\n",
        "!unzip $training_path\n",
        "!unzip $hidden_path\n",
        "!unzip $c3_training_path\n",
        "!unzip $c3_hidden_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KW4FNBBCt4E"
      },
      "source": [
        "Let's have a look at the dataset and how it is layed down.\n",
        "\n",
        "The data is structured as follows\n",
        "```\n",
        "c1_release/\n",
        "  images/\n",
        "    0001e6adc4fbab0c.jpg\n",
        "    00067fe83e3e21c8.jpg\n",
        "    0008ab3d8674f6ca.jpg\n",
        "    ...\n",
        "  labels.json\n",
        "  train.json\n",
        "  val.json\n",
        "```\n",
        "\n",
        "`labels.json` contains the labels for the whole dataset (`train.json` + `val.json`) if you need it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBZhr1inB_RU"
      },
      "source": [
        "### Defining the Dataset\n",
        "\n",
        "The [torchvision reference scripts for training object detection](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) allows for easily supporting adding new custom datasets.\n",
        "The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n",
        "\n",
        "The only specificity that we require is that the dataset `__getitem__` should return:\n",
        "\n",
        "* image: a PIL Image of size (H, W)\n",
        "* target: a dict containing the following fields\n",
        "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
        "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
        "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
        "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
        "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n",
        "\n",
        "If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\n",
        "\n",
        "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n",
        "\n",
        "Let's write a `torch.utils.data.Dataset` class for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Iv0DXlINyH"
      },
      "source": [
        "class TILDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, annotation, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.coco = COCO(annotation)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        cats = self.coco.loadCats(self.coco.getCatIds())\n",
        "        self.cat2name = {cat['id']:cat['name'] for cat in cats} # maps category id to category name (useful for visualization)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index] # Image ID\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id) # get annotation id from coco\n",
        "        coco_annotation = coco.loadAnns(ann_ids) # target coco_annotation file for an image\n",
        "        path = coco.loadImgs(img_id)[0]['file_name'] # path for input image\n",
        "        img = Image.open(os.path.join(self.root, 'images', path)).convert('RGB') # open the input image\n",
        "\n",
        "        # number of objects in the image\n",
        "        num_objs = len(coco_annotation)\n",
        "\n",
        "        # Bounding boxes for objects\n",
        "        # In coco format, bbox = [xmin, ymin, width, height]\n",
        "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            xmin = coco_annotation[i]['bbox'][0]\n",
        "            ymin = coco_annotation[i]['bbox'][1]\n",
        "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
        "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        # Labels\n",
        "        labels = []\n",
        "        for i in range(num_objs):\n",
        "            labels.append(coco_annotation[i]['category_id'])\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Tensorise img_id\n",
        "        img_id = torch.tensor([img_id])\n",
        "\n",
        "        # Size of bbox (Rectangular)\n",
        "        areas = []\n",
        "        for i in range(num_objs):\n",
        "            areas.append(coco_annotation[i]['area'])\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Annotation is in dictionary format\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = img_id\n",
        "        target[\"area\"] = areas\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWm6AvzjDdLs"
      },
      "source": [
        "That's all for the dataset. Let's see how the outputs are structured for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19AIZfTM4R4a"
      },
      "source": [
        "til_root = \"/content/c1_release/\" # extracted training dataset path\n",
        "til_root_hidden = \"/content/c1_hidden/\" # extracted training dataset path\n",
        "til_root_c3 = \"/content/c3_release/\" # extracted c3 dataset path\n",
        "til_hidden_c3 = \"/content/c3_hidden/\" # extracted c3 dataset path\n",
        "train_annotation = os.path.join(til_root, \"train.json\")\n",
        "hidden_annotation = os.path.join(til_root_hidden, \"labels.json\")\n",
        "c3_annotation = os.path.join(til_root_c3, \"train.json\")\n",
        "c3_hidden_annotation = os.path.join(til_root_c3, \"labels.json\")\n",
        "\n",
        "val_annotation = os.path.join(til_root, \"val.json\")\n",
        "val_c3_annotation = os.path.join(til_root_c3, \"val.json\")\n",
        "\n",
        "dataset = TILDataset(til_root, train_annotation)\n",
        "dataset[30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNUhbVfc4xVz"
      },
      "source": [
        "So we can see that by default, the dataset returns a `PIL.Image` and a dictionary containing several fields, including `boxes` and `labels`.\n",
        "\n",
        "Let's next look at one example from the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgvaYqPf6uyo"
      },
      "source": [
        "source_img, img_annots = dataset[30]\n",
        "draw = ImageDraw.Draw(source_img)\n",
        "for i in range(len(img_annots[\"boxes\"])):\n",
        "    x1, y1, x2, y2 = img_annots[\"boxes\"][i]\n",
        "    label = int(img_annots[\"labels\"][i])\n",
        "\n",
        "    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n",
        "    text = f'{dataset.cat2name[label]}'\n",
        "    draw.text((x1+5, y1+5), text)\n",
        "display(source_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OWoxpWcM3D4"
      },
      "source": [
        "## Setting up the Model\n",
        "\n",
        "In this object detection example, we will make use of Faster R-CNN model with a ResNet50-FPN backbone. To understand the underlying code structure, you can read this [article](https://zhuanlan.zhihu.com/p/145842317) (right click and translate to English).\n",
        "\n",
        "Feel free to explore with different hyper-parameters to see what works best!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c96yh1yaykGE"
      },
      "source": [
        "# hyper-parameters\n",
        "params = {'BATCH_SIZE': 16,\n",
        "          'LR': 0.005,\n",
        "          'CLASSES': 9,\n",
        "          'MAXEPOCHS': 10,\n",
        "          'BACKBONE': 'resnet152',\n",
        "          'FPN': True,\n",
        "          'ANCHOR_SIZE': ((32,), (64,), (128,), (256,), (512,)),\n",
        "          'ASPECT_RATIOS': ((0.5, 1.0, 2.0),),\n",
        "          'MIN_SIZE': 512,\n",
        "          'MAX_SIZE': 512,\n",
        "          'IMG_MEAN': [0.485, 0.456, 0.406],\n",
        "          'IMG_STD': [0.229, 0.224, 0.225],\n",
        "          'IOU_THRESHOLD': 0.5\n",
        "          }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0GWSdRqy9pA"
      },
      "source": [
        "def get_resnet_backbone(backbone_name: str):\n",
        "    \"\"\"\n",
        "    Returns a resnet backbone pretrained on ImageNet.\n",
        "    Removes the average-pooling layer and the linear layer at the end.\n",
        "    \"\"\"\n",
        "    if backbone_name == 'resnet18':\n",
        "        pretrained_model = models.resnet18(pretrained=True, progress=False)\n",
        "        out_channels = 512\n",
        "    elif backbone_name == 'resnet34':\n",
        "        pretrained_model = models.resnet34(pretrained=True, progress=False)\n",
        "        out_channels = 512\n",
        "    elif backbone_name == 'resnet50':\n",
        "        pretrained_model = models.resnet50(pretrained=True, progress=False)\n",
        "        out_channels = 2048\n",
        "    elif backbone_name == 'resnet101':\n",
        "        pretrained_model = models.resnet101(pretrained=True, progress=False)\n",
        "        out_channels = 2048\n",
        "    elif backbone_name == 'resnet152':\n",
        "        pretrained_model = models.resnet152(pretrained=True, progress=False)\n",
        "        out_channels = 2048\n",
        "\n",
        "    backbone = torch.nn.Sequential(*list(pretrained_model.children())[:-2])\n",
        "    backbone.out_channels = out_channels\n",
        "\n",
        "    return backbone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un4UIW_d8PEe"
      },
      "source": [
        "def get_resnet_fpn_backbone(backbone_name: str):\n",
        "    \"\"\"\n",
        "    Returns a specified ResNet backbone with FPN pretrained on ImageNet.\n",
        "    \"\"\"\n",
        "    return resnet_fpn_backbone(backbone_name, pretrained=True, trainable_layers=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5oH8VNty-kX"
      },
      "source": [
        "def get_anchor_generator(anchor_size: Tuple[tuple] = None, aspect_ratios: Tuple[tuple] = None):\n",
        "    \"\"\"Returns the anchor generator.\"\"\"\n",
        "    if anchor_size is None:\n",
        "        anchor_size = ((16,), (32,), (64,), (128,))\n",
        "    if aspect_ratios is None:\n",
        "        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_size)\n",
        "\n",
        "    anchor_generator = AnchorGenerator(sizes=anchor_size,\n",
        "                                       aspect_ratios=aspect_ratios)\n",
        "    return anchor_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj92yxjOzNft"
      },
      "source": [
        "def get_roi_pool(featmap_names: List[str] = None, output_size: int = 7, sampling_ratio: int = 2):\n",
        "    \"\"\"Returns the ROI Pooling\"\"\"\n",
        "    if featmap_names is None:\n",
        "        # default for resnet with FPN\n",
        "        featmap_names = ['0', '1', '2', '3']\n",
        "\n",
        "    roi_pooler = MultiScaleRoIAlign(featmap_names=featmap_names,\n",
        "                                    output_size=output_size,\n",
        "                                    sampling_ratio=sampling_ratio)\n",
        "\n",
        "    return roi_pooler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQNv6A4zzRqc"
      },
      "source": [
        "def get_fasterRCNN(backbone: torch.nn.Module,\n",
        "                   anchor_generator: AnchorGenerator,\n",
        "                   roi_pooler: MultiScaleRoIAlign,\n",
        "                   num_classes: int,\n",
        "                   image_mean: List[float] = [0.485, 0.456, 0.406],\n",
        "                   image_std: List[float] = [0.229, 0.224, 0.225],\n",
        "                   min_size: int = 512,\n",
        "                   max_size: int = 1024,\n",
        "                   **kwargs\n",
        "                   ):\n",
        "    \"\"\"Returns the Faster-RCNN model. Default normalization: ImageNet\"\"\"\n",
        "    model = FasterRCNN(backbone=backbone,\n",
        "                       rpn_anchor_generator=anchor_generator,\n",
        "                       box_roi_pool=roi_pooler,\n",
        "                       num_classes=num_classes,\n",
        "                       image_mean=image_mean,  # ImageNet\n",
        "                       image_std=image_std,  # ImageNet\n",
        "                       min_size=min_size,\n",
        "                       max_size=max_size,\n",
        "                       **kwargs\n",
        "                       )\n",
        "    model.num_classes = num_classes\n",
        "    model.image_mean = image_mean\n",
        "    model.image_std = image_std\n",
        "    model.min_size = min_size\n",
        "    model.max_size = max_size\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfn_4EbLzfHf"
      },
      "source": [
        "def get_fasterRCNN_resnet(num_classes: int,\n",
        "                          backbone_name: str,\n",
        "                          anchor_size: List[float],\n",
        "                          aspect_ratios: List[float],\n",
        "                          fpn: bool = True,\n",
        "                          min_size: int = 512,\n",
        "                          max_size: int = 1024,\n",
        "                          **kwargs\n",
        "                          ):\n",
        "    \"\"\"Returns the Faster-RCNN model with resnet backbone with and without fpn.\"\"\"\n",
        "\n",
        "    # Backbone\n",
        "    if fpn:\n",
        "        backbone = get_resnet_fpn_backbone(backbone_name=backbone_name)\n",
        "    else:\n",
        "        backbone = get_resnet_backbone(backbone_name=backbone_name)\n",
        "\n",
        "    # Anchors\n",
        "    anchor_size = anchor_size\n",
        "    aspect_ratios = aspect_ratios * len(anchor_size)\n",
        "    anchor_generator = get_anchor_generator(anchor_size=anchor_size, aspect_ratios=aspect_ratios)\n",
        "\n",
        "    # ROI Pool\n",
        "    with torch.no_grad():\n",
        "        backbone.eval()\n",
        "        random_input = torch.rand(size=(1, 3, 512, 512))\n",
        "        features = backbone(random_input)\n",
        "\n",
        "    if isinstance(features, torch.Tensor):\n",
        "        from collections import OrderedDict\n",
        "\n",
        "        features = OrderedDict([('0', features)])\n",
        "\n",
        "    featmap_names = [key for key in features.keys() if key.isnumeric()]\n",
        "\n",
        "    roi_pool = get_roi_pool(featmap_names=featmap_names)\n",
        "\n",
        "    # Model\n",
        "    return get_fasterRCNN(backbone=backbone,\n",
        "                          anchor_generator=anchor_generator,\n",
        "                          roi_pooler=roi_pool,\n",
        "                          num_classes=num_classes,\n",
        "                          min_size=min_size,\n",
        "                          max_size=max_size,\n",
        "                          **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hljmqSJOzibW"
      },
      "source": [
        "model = get_fasterRCNN_resnet(num_classes=params['CLASSES'],\n",
        "                              backbone_name=params['BACKBONE'],\n",
        "                              anchor_size=params['ANCHOR_SIZE'],\n",
        "                              aspect_ratios=params['ASPECT_RATIOS'],\n",
        "                              fpn=params['FPN'],\n",
        "                              min_size=params['MIN_SIZE'],\n",
        "                              max_size=params['MAX_SIZE'],\n",
        "                              image_mean=params['IMG_MEAN'],\n",
        "                              image_std=params['IMG_STD'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpG3AhNjOJ6-"
      },
      "source": [
        "# load pretrained weights for FasterRCNN ResNet50 FPN\n",
        "pretrained_dict = torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth', progress=True)\n",
        "model_dict = model.state_dict()\n",
        "\n",
        "# filter out roi_heads.box_predictor weights\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if not k.startswith('roi_heads.box_predictor')}\n",
        "# overwrite entries in the existing state dict\n",
        "model_dict.update(pretrained_dict)\n",
        "# load the new state dict\n",
        "model.load_state_dict(model_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dacuDIn3z69Y"
      },
      "source": [
        "# move model to the right device\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iPlWCjV28P5"
      },
      "source": [
        "# construct an optimizer\n",
        "model_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(model_params, \n",
        "                            lr=params['LR'],\n",
        "                            momentum=0.9, \n",
        "                            weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3_rz0d4Khpn"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "Let's write some helper functions for data augmentation / transformation.\n",
        "\n",
        "Do not just stop here, add in your own data augmentations! Remember to also augment the bounding boxes accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmr8kmOZhJ91"
      },
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNeXIW8shR0_"
      },
      "source": [
        "# converts the image, a PIL image, into a PyTorch Tensor\n",
        "class ToTensor(object):\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwFX52gbhUQX"
      },
      "source": [
        "# randomly horizontal flip the images and ground-truth labels\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = image.flip(-1)\n",
        "            bbox = target[\"boxes\"]\n",
        "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "            target[\"boxes\"] = bbox\n",
        "        return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOWwmYedO32b"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class RandomGrayScale(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            pil = transforms.ToPILImage()(image).convert(\"RGB\")\n",
        "            pil = torchvision.transforms.functional.to_grayscale(pil)\n",
        "            image = F.to_tensor(pil)\n",
        "            bbox = target[\"boxes\"]\n",
        "            target[\"boxes\"] = bbox\n",
        "        return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csqqwmmfZ-Zg"
      },
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "class RandomPerspective(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "      pil = transforms.ToPILImage()(image).convert(\"RGB\")\n",
        "      pil = torchvision.transforms.RandomInvert(p=0.5).forward(pil)\n",
        "      image = F.to_tensor(pil)\n",
        "      bbox = target[\"boxes\"]\n",
        "      target[\"boxes\"] = bbox\n",
        "\n",
        "      return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4E-e3zLVHwL"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class RandomBlur(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "      pil = transforms.ToPILImage()(image).convert(\"RGB\")\n",
        "      pil = torchvision.transforms.RandomAdjustSharpness(sharpness_factor=0, p=self.prob).forward(pil)\n",
        "      image = F.to_tensor(pil)\n",
        "      bbox = target[\"boxes\"]\n",
        "      target[\"boxes\"] = bbox\n",
        "\n",
        "      return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkL76-PE5Jvq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omm4z5GsJ-wS"
      },
      "source": [
        "def get_transform(train):\n",
        "    if train:\n",
        "        transforms = Compose([\n",
        "            ToTensor(), \n",
        "            RandomHorizontalFlip(0.5),\n",
        "            RandomGrayScale(0.5),\n",
        "            RandomBlur(0.5),\n",
        "            RandomPerspective(0.5)\n",
        "        ])\n",
        "    else: # during evaluation, no augmentations will be done\n",
        "        transforms = Compose([\n",
        "            ToTensor()\n",
        "        ])\n",
        "    \n",
        "    return transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqp_tYw1IXvI"
      },
      "source": [
        "Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled by the R-CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9hhlhDDMEos"
      },
      "source": [
        "## Data Loaders\n",
        "\n",
        "Let's now set up our data loaders so that we can streamline the batch loading of data for our model training later on.\n",
        "\n",
        "We now have the dataset class, the models and the data transforms. Let's instantiate them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DBaaGV7JP1W"
      },
      "source": [
        "NUM_WORKERS = 4\n",
        "\n",
        "# use our dataset and defined transformations\n",
        "train_dataset = TILDataset(til_root, train_annotation, get_transform(train=True))\n",
        "hidden_dataset = TILDataset(til_root_hidden, hidden_annotation, get_transform(train=True))\n",
        "c3_train_dataset = TILDataset(til_root_c3, c3_annotation, get_transform(train=True))\n",
        "c3_hidden_dataset = TILDataset(til_hidden_c3, c3_hidden_annotation, get_transform(train=True))\n",
        "\n",
        "val_dataset = TILDataset(til_root, val_annotation, get_transform(train=False))\n",
        "c3_val_dataset = TILDataset(til_root_c3, val_c3_annotation, get_transform(train=False))\n",
        "\n",
        "# define training and validation data loaders\n",
        "#train_loader = torch.utils.data.DataLoader(\n",
        "#    train_dataset, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=NUM_WORKERS,\n",
        "#    collate_fn=utils.collate_fn)\n",
        "\n",
        "concatDataSet = torch.utils.data.ConcatDataset([train_dataset, hidden_dataset, c3_train_dataset, c3_hidden_dataset])\n",
        "concatValDataSet = torch.utils.data.ConcatDataset([val_dataset, c3_val_dataset])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    concatDataSet, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=NUM_WORKERS,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    concatValDataSet, batch_size=1, shuffle=False, num_workers=NUM_WORKERS,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0amZn1wMHvkP"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "And now let's train the model, evaluating at the end of every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnNPceNj5vTc"
      },
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1. / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses, **loss_dict)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8uKl51Z506h"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Test:'\n",
        "\n",
        "    coco = data_loader.dataset.coco\n",
        "    iou_types = [\"bbox\"]\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for image, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        image = list(img.to(device) for img in image)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(image)\n",
        "\n",
        "        outputs = [{k: v for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trjdd2wgXqfv"
      },
      "source": [
        "for epoch in range(params['MAXEPOCHS']):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
        "\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, val_loader, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACQjk3ekP7cc"
      },
      "source": [
        "## Visualization of results\n",
        "\n",
        "Now that training has finished, let's have a look at what it actually predicts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHwIdxH76uPj"
      },
      "source": [
        "# pick one image from the validation set\n",
        "img, _ = val_dataset[391]\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])\n",
        "\n",
        "prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omW9nKNYPOmr"
      },
      "source": [
        "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
        "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, and `scores` as fields.\n",
        "\n",
        "Let's inspect the image and the predicted boxes.\n",
        "For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yQom5VmPg0G"
      },
      "source": [
        "# convert the image, which has been rescaled to 0-1 and had the channels flipped\n",
        "pred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
        "draw = ImageDraw.Draw(pred_img)\n",
        "\n",
        "img_preds = prediction[0]\n",
        "for i in range(len(img_preds[\"boxes\"])):\n",
        "    x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n",
        "    label = int(img_preds[\"labels\"][i])\n",
        "    score = float(img_preds[\"scores\"][i])\n",
        "\n",
        "    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n",
        "    text = f'{dataset.cat2name[label]}: {score}'\n",
        "    draw.text((x1+5, y1+5), text)\n",
        "\n",
        "display(pred_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyVWaxdXQEMI"
      },
      "source": [
        "## Post-processing\n",
        "\n",
        "We might notice that there are duplicate detections in the image. Let's post-process the detections with non-maximum suppression.\n",
        "\n",
        "** Update: FasterRCNN already has NMS built into it, so you actually do not need to do NMS again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmui7x3zQOvY"
      },
      "source": [
        "img_preds = prediction[0]\n",
        "keep_idx = batched_nms(boxes=img_preds[\"boxes\"], scores=img_preds[\"scores\"], idxs=img_preds[\"labels\"], iou_threshold=params['IOU_THRESHOLD'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kctZV8vQbLD"
      },
      "source": [
        "Check the predictions again after applying nms.\n",
        "\n",
        "** Update: You should not see any difference unless you have specified a lower IoU threshold than the default of 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEYw-SWxQclI"
      },
      "source": [
        "# convert the image, which has been rescaled to 0-1 and had the channels flipped\n",
        "pred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
        "draw = ImageDraw.Draw(pred_img)\n",
        "\n",
        "for i in range(len(img_preds[\"boxes\"])):\n",
        "    if i in keep_idx:\n",
        "        x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n",
        "        label = int(img_preds[\"labels\"][i])\n",
        "        score = float(img_preds[\"scores\"][i])\n",
        "\n",
        "        draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n",
        "        text = f'{dataset.cat2name[label]}: {score}'\n",
        "        draw.text((x1+5, y1+5), text)\n",
        "\n",
        "display(pred_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1vTbbN2Q1Ys"
      },
      "source": [
        "Now, let's further filter out the non-confident detections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpfDgkUzRCSy"
      },
      "source": [
        "det_threshold = 0.5\n",
        "\n",
        "# convert the image, which has been rescaled to 0-1 and had the channels flipped\n",
        "pred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
        "draw = ImageDraw.Draw(pred_img)\n",
        "\n",
        "for i in range(len(img_preds[\"boxes\"])):\n",
        "    if i in keep_idx:\n",
        "        x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n",
        "        label = int(img_preds[\"labels\"][i])\n",
        "        score = float(img_preds[\"scores\"][i])\n",
        "\n",
        "        # filter out non-confident detections\n",
        "        if score > det_threshold:\n",
        "            draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n",
        "            text = f'{dataset.cat2name[label]}: {score}'\n",
        "            draw.text((x1+5, y1+5), text)\n",
        "\n",
        "display(pred_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjsr7fSkRKbB"
      },
      "source": [
        "Much better! Once you are satisfied with the results, save your model weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lycezchIRPK7"
      },
      "source": [
        "# save model weights\n",
        "save_path = os.path.join(base_folder, \"c1_weights.pth\")\n",
        "torch.save(model.state_dict(), save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivhZ0aMlg_nF"
      },
      "source": [
        "## Evaluation on Validation Set\n",
        "\n",
        "As a sanity check, let's evaluate the model performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCbo_MyRhEVK"
      },
      "source": [
        "with open(val_annotation) as json_file:\n",
        "    val_data = json.load(json_file)\n",
        "\n",
        "model.eval()\n",
        "detections = []\n",
        "with torch.no_grad():\n",
        "    for image in val_data[\"images\"]:\n",
        "        img_name = image[\"file_name\"]\n",
        "        img_id = image[\"id\"]\n",
        "\n",
        "        img = Image.open(os.path.join(til_root, 'images', img_name)).convert('RGB')\n",
        "        img_tensor = transforms.ToTensor()(img)\n",
        "\n",
        "        preds = model([img_tensor.to(device)])[0]\n",
        "\n",
        "        for i in range(len(preds[\"boxes\"])):\n",
        "            x1, y1, x2, y2 = preds[\"boxes\"][i]\n",
        "            label = int(preds[\"labels\"][i])\n",
        "            score = float(preds[\"scores\"][i])\n",
        "\n",
        "            left = int(x1)\n",
        "            top = int(y1)\n",
        "            width = int(x2 - x1)\n",
        "            height = int(y2 - y1)\n",
        "\n",
        "            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aysXD7DwjFSs"
      },
      "source": [
        "validation_json = os.path.join(base_folder, \"validation_preds.json\")\n",
        "with open(validation_json, 'w') as f:\n",
        "    json.dump(detections, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymAERJJhcc_"
      },
      "source": [
        "# Get evaluation score against validation set to make sure your prediction json file is in the correct format\n",
        "coco_gt = COCO(val_annotation)\n",
        "coco_dt = coco_gt.loadRes(validation_json)\n",
        "cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAIVOKR2pgx2"
      },
      "source": [
        "## Evaluation on Validation Set\n",
        "\n",
        "As a sanity check, let's evaluate the model performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj3_d6-bpgx2"
      },
      "source": [
        "with open(val_annotation) as json_file:\n",
        "    val_data = json.load(json_file)\n",
        "\n",
        "model.eval()\n",
        "detections = []\n",
        "with torch.no_grad():\n",
        "    for image in val_data[\"images\"]:\n",
        "        img_name = image[\"file_name\"]\n",
        "        img_id = image[\"id\"]\n",
        "\n",
        "        img = Image.open(os.path.join(til_root, 'images', img_name)).convert('RGB')\n",
        "        img_tensor = transforms.ToTensor()(img)\n",
        "\n",
        "        preds = model([img_tensor.to(device)])[0]\n",
        "\n",
        "        for i in range(len(preds[\"boxes\"])):\n",
        "            x1, y1, x2, y2 = preds[\"boxes\"][i]\n",
        "            label = int(preds[\"labels\"][i])\n",
        "            score = float(preds[\"scores\"][i])\n",
        "\n",
        "            left = int(x1)\n",
        "            top = int(y1)\n",
        "            width = int(x2 - x1)\n",
        "            height = int(y2 - y1)\n",
        "\n",
        "            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3iMnZ0Mpgx3"
      },
      "source": [
        "validation_json = os.path.join(base_folder, \"validation_preds.json\")\n",
        "with open(validation_json, 'w') as f:\n",
        "    json.dump(detections, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1mU9Onmpgx3"
      },
      "source": [
        "# Get evaluation score against validation set to make sure your prediction json file is in the correct format\n",
        "coco_gt = COCO(val_annotation)\n",
        "coco_dt = coco_gt.loadRes(validation_json)\n",
        "cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUGlO1lFRQQd"
      },
      "source": [
        "## Generate Predictions on Test Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vlMBqKgVUjD"
      },
      "source": [
        "# You should already have uploaded the testing images onto your google drive.\n",
        "\n",
        "testing_images_path = os.path.join(base_folder, \"c1_test_release.zip\") # change this path to the zip file for the test images\n",
        "!unzip $testing_images_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SeC-BIoWYT-"
      },
      "source": [
        "# You should already have uploaded the c3 testing images onto your google drive.\n",
        "\n",
        "c3_testing_images_path = os.path.join(base_folder, \"c3_test_release.zip\") # change this path to the zip file for the test images\n",
        "!unzip $c3_testing_images_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VImfhYxKuvKz"
      },
      "source": [
        "til_test_root = \"/content/c1_test_release/\" # extracted testing images path\n",
        "c3_til_test_root = \"/content/c3_test_release/\" # extracted testing images path\n",
        "test_img_root = os.path.join(til_test_root, \"images\")\n",
        "c3_test_img_root = os.path.join(c3_til_test_root, \"images\")\n",
        "img_dir = os.scandir(test_img_root)\n",
        "c3_img_dir = os.scandir(c3_test_img_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7QnHBguS4qZ"
      },
      "source": [
        "# load model weights (if not using the current trained model)\n",
        "model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1dODyYiTXSF"
      },
      "source": [
        "Let's visualize some predictions on the test images. Run this a few times to visualize different images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBwlAvrCTZgU"
      },
      "source": [
        "img = Image.open(next(img_dir).path).convert('RGB')\n",
        "draw = ImageDraw.Draw(img)\n",
        "det_threshold = 0.5\n",
        "\n",
        "# do the prediction\n",
        "with torch.no_grad():\n",
        "    img_tensor = transforms.ToTensor()(img)\n",
        "    img_preds = model([img_tensor.to(device)])[0]\n",
        "\n",
        "for i in range(len(img_preds[\"boxes\"])):\n",
        "    x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n",
        "    label = int(img_preds[\"labels\"][i])\n",
        "    score = float(img_preds[\"scores\"][i])\n",
        "\n",
        "    # filter out non-confident detections\n",
        "    if score > det_threshold:\n",
        "        draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n",
        "        text = f'{dataset.cat2name[label]}: {score}'\n",
        "        draw.text((x1+5, y1+5), text)\n",
        "\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeGHrPzIa9_7"
      },
      "source": [
        "## Submission of Results\n",
        "\n",
        "Submission json file should be in [COCO format](https://cocodataset.org/#format-results).\n",
        "\n",
        "```\n",
        "[{\n",
        "    \"image_id\": int, \n",
        "    \"category_id\": int, \n",
        "    \"bbox\": [x,y,width,height], \n",
        "    \"score\": float,\n",
        "}]\n",
        "```\n",
        "\n",
        "Refer to **sample_submission_cv.json** for an example.\n",
        "\n",
        "For this competition, the metric for evaluation will be mAP @ 0.50:0.95"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJqFPKWITzgo"
      },
      "source": [
        "# generate detections on the folder of test images (this will be used for submission)\n",
        "detections = []\n",
        "with torch.no_grad():\n",
        "    for image in c3_img_dir:\n",
        "        img_id = int(image.name.split('.')[0])\n",
        "\n",
        "        img = Image.open(image.path).convert('RGB')\n",
        "        img_tensor = transforms.ToTensor()(img)\n",
        "\n",
        "        preds = model([img_tensor.to(device)])[0]\n",
        "\n",
        "        for i in range(len(preds[\"boxes\"])):\n",
        "            x1, y1, x2, y2 = preds[\"boxes\"][i]\n",
        "            label = int(preds[\"labels\"][i])\n",
        "            score = float(preds[\"scores\"][i])\n",
        "\n",
        "            left = int(x1)\n",
        "            top = int(y1)\n",
        "            width = int(x2 - x1)\n",
        "            height = int(y2 - y1)\n",
        "\n",
        "            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGPpF_F7QX98"
      },
      "source": [
        "test_pred_json = os.path.join(base_folder, \"test_preds.json\")\n",
        "with open(test_pred_json, 'w') as f:\n",
        "    json.dump(detections, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkJTEJ6mlxgz"
      },
      "source": [
        "## Check Your Submission JSON Format (RUN THIS BEFORE SUBMISSION)\n",
        "\n",
        "Run this function first with the given sample json (change this path for the different challenges) to make sure everything works before you submit. Just need to ensure that there are no errors when you run this. **If you get any errors, check your generated json file.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ugbAD9RXW2E"
      },
      "source": [
        "sample_json_path = os.path.join(til_test_root, \"c3_test_sample.json\")\n",
        "\n",
        "coco_gt = COCO(sample_json_path)\n",
        "coco_dt = coco_gt.loadRes(test_pred_json)\n",
        "cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ1KgDOGlw8n"
      },
      "source": [
        "If you get an `AssertionError: Results do not correspond to current coco set`, it most likely means that some of the \"image_id\" are out of range (either 0 or higher than number of test images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4MLUw05l4VR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}